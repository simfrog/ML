# ch2-1. 선형 회귀(Linear Regression)의 가설(Hypothesis)와 비용(cost)  

## 용어  
### 1. 회귀분석  
점들이 퍼져있는 형태에서 패턴을 찾아내고, 이 패턴을 활용해서 무언가를 예측하는 분석  
새로운 표본을 뽑았을 때 평균으로 돌아가려는 특징이 있기 때문에 붙은 이름  
### 2. Linear Regression(선형 회귀)    
2차원 좌표에 분포된 데이터를 1차원 직선 방정식을 통해 표현되지 않은 데이터를 예측하기 위한 분석 모델  
가장 기본적이고 많이 사용되는 머신러닝 알고리즘 중 하나  
Training data를 이용하여 데이터의 특성과 상관관계 등을 파악하고, 그 결과를 바탕으로  
Training data에 없는 미지의 데이터가 주어졌을 경우 그 결과를 연속적인 (숫자)값으로 예측하는 것  
ex) 시험 공부하는 시간을 늘리면 성적이 잘 나옴  
![regression1](https://user-images.githubusercontent.com/31130917/107740649-faf26b80-6d4e-11eb-8e46-78953e432a91.PNG)  
### 3. Hypothesis  
Linear Regression에서 사용한는 1차원 방정식을 가리키는 용어  
H(x)로 표현  
#### H(x) = Wx + b  
기울기인 W(Weight)와 절편인 b(bias)가 반복되는 과정에서 계속 바뀌고, 마지막 루프에서 바뀐 최종 값을 사용해서 데이터 예측에 사용  
최종 결과로 나온 가설을 모델(model)이라고 하고, "학습되었다"라고 표현  
### 4. Cost(비용)  
Hypothesis 방정식에 대한 cost로 방정식의 결과가 크게 나오면 좋지 않다고 표현, 루프를 돌 때마다 W와 b를 비용이 적게 발생하는 방향으로 수정  
### 5. Cost 함수  
Hypothesis 방정식을 포함하는 계산식, 현재의 W와 b에 대해 비용을 계산해 주는 함수  

## 1. Hypothesis  
* ### (Linear) Hypothesis  
![선형회귀](https://user-images.githubusercontent.com/31130917/107740645-f9c13e80-6d4e-11eb-94d6-23b54f76c555.PNG)  
보기에는 파란색 직선이 가장 좋은 직선인 것을 알지만, 어떤 데이터가 있는지 모르기 때문에 단정지을 수 없음  
  
![어느가설이좋은가](https://user-images.githubusercontent.com/31130917/107740932-88ce5680-6d4f-11eb-87ee-e1f72643d40c.PNG)  
위 그림이 파란색 직선은 노랑이거나 빨강일 수 도 있음, 실제로는 모든 직선이라고 보면 됨  
데이터에 가장 가깝게 그려지는 직선을 찾는 것이 목표  
#### 직선으로 부터 각각의 데이터(좌표)까지의 거리 합계를 계산한 것을 cost라 부름  
이 값이 가장 작은 직선을 찾으면 목표 달성  
    점과 y절편의 차이를 오차(error)라 함  
    오차(error) = H(x) - y = (Wx+b) - y  
    오차가 작다면 예측도 정확할 수 있다고 예상할 수 있음  
  
* ### cost function(Loss function)  
H(x)-y 를 모두 더해서 손실함수를 구하면 각각의 오차가 +, - 등이 동시에 존해하기 때문에 오차의 합이 0이 나올 수 있음  
0은 최소 오차 값인지 아닌지 판별하는 것이 어려움  
### => (H(x)-y)^2 를 사용  
#### => 오차는 언제나 양수이며, 정답과 계산값의 차이가 크다면 제곱에 의해 오차는 더 큰 값을 가지게 되어 학습에 있어 큰 장점  
  
비용 계산은 다음과 같음  
![비용](https://user-images.githubusercontent.com/31130917/107741332-4d805780-6d50-11eb-8f29-d7e6213617b8.PNG)  
#### m은 데이터 개수, H(x)는 예측값, y는 실제값  
  
![비용정리](https://user-images.githubusercontent.com/31130917/107741538-b7006600-6d50-11eb-9131-7ed853a43450.PNG)  
x와 y는 training data에서 주어지는 값이므로,  
#### 손실함수 cost(W,b)는 W와 b에 영향을 받는 함수  
cost(W,b)값이 작다는 것은 평균 오차가 작다는 의미 -> 확률적으로 예측값도 오차가 작을 것이라고 추측할 수 있음  
### => Goal : Minimize cost  
<출처 : 모두를 위한 딥러닝>
