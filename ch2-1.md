# ch2-1. 선형 회귀(Linear Regression)의 가설(Hypothesis)와 비용(cost)  

## 1. 선형 회귀(Linear Regression)  
가장 기본적이고 많이 사용되는 머신러닝 알고리즘 중 하나  
어떤 요인의 수치에 따라 특정 요인의 수치가 영향을 받음  
Training data를 이용하여 데이터의 특성과 상관관계 등을 파악하고, 그 결과를 바탕으로  
Training data에 없는 미지의 데이터가 주어졌을 경우 그 결과를 연속적인 (숫자)값으로 예측하는 것  
#### 변수 x에 의해서 값이 종속적으로 변하는 변수 y  
ex) 시험 공부하는 시간을 늘리면 성적이 잘 나옴  
![regression1](https://user-images.githubusercontent.com/31130917/107740649-faf26b80-6d4e-11eb-8e46-78953e432a91.PNG)  
  
* ### 학습(learning)  
1. training data 분석  
  #### 입력(x)과 출력(y)은 y = Wx + b 형태로 나타낼 수 있음
2. W 와 b 찾기  
  training data의 특성을 가장 잘 표현 할 수 있는  
  #### 가중치 W(기울기), 바이어스 b(y절편)를 찾는 것이 학습의 개념  
  
* ### (Linear) Hypothesis  
![선형회귀](https://user-images.githubusercontent.com/31130917/107740645-f9c13e80-6d4e-11eb-94d6-23b54f76c555.PNG)  
  
![어느가설이좋은가](https://user-images.githubusercontent.com/31130917/107740932-88ce5680-6d4f-11eb-87ee-e1f72643d40c.PNG)  
#### => 점과 y절편의 차이를 오차(error)라 함  
#### => 오차(error) = H(x) - y = (Wx+b) - y  
#### => 오차가 작다면 예측도 정확할 수 있다고 예상할 수 있음  
  
* ### cost function(Loss function)  
H(x)-y 를 모두 더해서 손실함수를 구하면 각각의 오차가 +, - 등이 동시에 존해하기 때문에 오차의 합이 0이 나올 수 있음  
0은 최소 오차 값인지 아닌지 판별하는 것이 어려움  
### => (H(x)-y)^2 를 사용  
#### => 오차는 언제나 양수이며, 정답과 계산값의 차이가 크다면 제곱에 의해 오차는 더 큰 값을 가지게 되어 학습에 있어 큰 장점  
  
비용 계산은 다음과 같음  
![비용](https://user-images.githubusercontent.com/31130917/107741332-4d805780-6d50-11eb-8f29-d7e6213617b8.PNG)  
#### m은 데이터 개수, H(x)는 예측값, y는 실제값  
  
![비용정리](https://user-images.githubusercontent.com/31130917/107741538-b7006600-6d50-11eb-9131-7ed853a43450.PNG)  
x와 y는 training data에서 주어지는 값이므로,  
#### 손실함수 cost(W,b)는 W와 b에 영향을 받는 함수  
cost(W,b)값이 작다는 것은 평균 오차가 작다는 의미 -> 확률적으로 예측값도 오차가 작을 것이라고 추측할 수 있음  
### => Goal : Minimize cost  
<출처 : 모두를 위한 딥러닝>
