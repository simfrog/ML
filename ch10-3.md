# ch10-3. NN dropout & model 앙상블

## 1. Overfitting 해결방법
![overfitting](https://user-images.githubusercontent.com/31130917/111351306-64ef8f00-86c6-11eb-8903-412aeffe5e5e.png)  
#### layer가 많아질 때 발생하는 대표적인 문제점 overfitting
왼쪽 그림처럼 정확하게 분류하려 하지 말고 어느 정도의 오차를 허용해야 하는데, 오른쪽 그림처럼 선을 구부려서 너무 잘 분류가 되는 것처럼 보일 때 overfitting 이라고 함  
  
![overfitting 확인](https://user-images.githubusercontent.com/31130917/111351715-cf083400-86c6-11eb-8c51-513819364ae4.png)  
위 그림에서 파란색으로 그려신 선은 training set에 대해 동작하는 error, 빨간선은 실제 데이터에 대해 동작하는 error 나타냄, x축은 weight(layer), y축은 error  
overfitting된 모델은 layer 개수가 어느 정도일 때까지는 잘 동작하지만, 그 선을 넘어가는 순간부터 오히려 error가 증가  
  
* ### overfitting을 방지하는 3가지 방법  
#### 1) training data을 많이 모으기
데이터가 많으면 training set, validation set, test set으로 나눠서 진행할 수 있고, 영역별 데이터의 크기도 커지기 때문에 overfitting 확률 낮아짐(overfitting을 판단하기가 쉬워지기 때문에 정확한 모델을 만들 수 있다고 볼 수도 있음)  
#### 2) feature 개수 줄이기
서로 비중이 다른 feature가 섞여서 weight에 대해 경합을 하면 오히려 좋지 않은 결과가 나옴  
하지만 이 부분은 deep learning에서는 중요하지 않음, deep learning은 sigmoid 대신 ReLU 함수 계열을 사용하고 후에 나오는 dropout을 통해 feature를 스스로 줄일 수 있음  
#### 3) Regularization
weight이 너무 큰 값을 갖지 못하도록 제한하기 위한 방법  
선이 구부러지는 형태를 피할 수 있음  
![regularizaiton](https://user-images.githubusercontent.com/31130917/111353686-e34d3080-86c8-11eb-84ad-df8af215fb83.png)  
위 그림은 Regularization에서 가장 많이 사용하는 l2reg(엘투 regularization)를 보여줌  
남다값을 보통 0.001로 주는데, 중요하게 생각한다면 0.01도 가능하고, 너무너무 중요하다면 0.1로 줄 수 도 있음  
(deep learning에서 overfitting을 막기 위한 좋은 방법)  
남다의 값을 0.001로 준다면 1/1000만 반영한다는 것인데, weight의 제곱 합계이기 때문에 이 정도만으로도 충분한 영향력을 행사할 수 있음  
  
## 2. Dropout
![dropout](https://user-images.githubusercontent.com/31130917/111354334-8c942680-86c9-11eb-81ea-2dad9e9f1380.png)  
#### Dropout : 전체 weight을 계산에 참여시키는 것이 아니라 layer에 포함된 weight 중에서 일부만 참여 시키는 것
#### forward pass로 데이터를 전달할 때, 난수를 사용해서 일부 뉴런을 0으로 만드는 방법(0으로 만들면 제외한 것과 같은 효과)
전문가들이 너무 많다고 가정했을 때 너무 많은 weight이 있다면, 이들 중 일부만 사용해도 충분히 결과를 낼 수 있음  
오히려 이들 중에서 충분할 만큼의 전문가만 선출해서 반복적으로 결과를 낸다면, 균형잡힌 훌륭한 결과가 나올 수 있음  
  
## 3. Ensenble(앙상블)
![앙상블](https://user-images.githubusercontent.com/31130917/111357909-1691be80-86cd-11eb-9a9c-483392a15e36.png)  
#### overfitting을 피한다 => 실제 데이터에 대해 좋은 결과를 낸다는 말
#### 데이터가 많고 컴퓨터도 많다면 앙상블 방법도 가능
#### Ensenble : 데이터를 여러개의 training set으로 나눠서 동시에 학습을 진행시켜 모든 training set에 대한 학습이 끝나면 결과를 통합하는 방법
이 방법을 사용하면 최소 2%에서 4~5%까지의 성능이 향상  
<출처 : 모두를 위한 딥러닝>
