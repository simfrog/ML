# ch10-2. Initialize weights

## 1. RBM
### 지금까지 우리는 잘못된 방법으로 wieght을 초기화했음(hinton 교수님의 정리한 4가지 중)
  
![cost function](https://user-images.githubusercontent.com/31130917/111317458-89883e80-86a7-11eb-8ced-3c8c187b7dfd.png)  
똑같은 코드에 대해서 난수로 초기화를 하기 때문에 ReLU에 대해서도 2가지 결과가 나옴  
#### 어떤 초기값을 선택하느냐에 따라 매번 달라지고, 성능이 좋기도 나쁘기도 함  
  
weight에 대한 초기값을 모두 0으로 설정한다면?  
=> 딥러닝 알고리즘은 전혀 동작하지 않을 것  
  
![rbm structure](https://user-images.githubusercontent.com/31130917/111318057-0ca99480-86a8-11eb-81e3-2297bff8e472.png)  
위 그림은 RBM의 구조  
#### 같은 layer 안에서 어떠한 연결도 존재하지 않음
#### 단지 2개의 layer에 대해서만 x값을 사용해서 동작하고 y값도 사용하지 않는 초기화 방법
  
![rbm input](https://user-images.githubusercontent.com/31130917/111318061-0d422b00-86a8-11eb-8114-8ecf21cc870b.png)  
#### RBM은 현재 layer와 다음 layer에 대해서만 동작  
#### forward -> 현재 layer에 들어온 x값에 대해 weight을 계산한 값을 다음 layer에 전달
#### backward -> 전달받은 값을 이번에는 거꾸로 이전 layer에 weight 값을 계산해서 전달
#### forward와 backward 계산을 반복해서 진행하면, 최초 전달된 x와 예측한 값(x hat)의 차이가 최소가 되는 weight을 발견하게 됨
  
### RBM : 앞에서 설명한 내용을 2개 layer에 대해 초기값을 결정할 때까지 반복하고, 다음번 2개에 대해 다시 반복하고,  
### 이것을 마지막 layer까지 반복하는 방식
### 이렇게해서 초기화된 모델을 Deep Belief Network라고 부름
  
![pretraining](https://user-images.githubusercontent.com/31130917/111320477-5abf9780-86aa-11eb-9967-3c6802b10de0.png)
![221A104B57A0076A2D](https://user-images.githubusercontent.com/31130917/111320484-5c895b00-86aa-11eb-9e76-dd50133454a1.png)
![2414D44B57A0076C32](https://user-images.githubusercontent.com/31130917/111320486-5c895b00-86aa-11eb-86c5-2113197ffc9a.png)  
#### 이러한 과정을 pre-training이라고 부름
  
![전체 weigth 초기화](https://user-images.githubusercontent.com/31130917/111319111-02d46100-86a9-11eb-9488-85f0cf01c3ab.png)  
네트워크 전체의 weight이 초기화  
  
![fine tuning](https://user-images.githubusercontent.com/31130917/111319264-2697a700-86a9-11eb-9479-0d012aaa7ef5.png)  
제대로 초기화해서 학습을 시작하면 학습시간이 얼마 걸리지 않음  
#### 학습(learning)이라고 부르지 않고 Fine Tuning이라고 부름
  
## 2. 기타 초기화 방법
![xavier_he](https://user-images.githubusercontent.com/31130917/111319456-5d6dbd00-86a9-11eb-8d3e-64ce813b27a5.png)  
#### xavier(자비에르)와 He는 매우 단순한 초기화 방법(xavier를 응용한 것이 He)
  
* #### xavier : 입력값과 출력값 사이의 난수를 선택해서 입력값의 제곱근으로 나눔  
* #### he : 입력값을 반으로 나눈 제곱근 사용, 분모가 작아지기 때문에 xavier보다 넓은 범위의 난수 생성  
  
![기타](https://user-images.githubusercontent.com/31130917/111319821-bdfcfa00-86a9-11eb-954f-6a6b2e6bad3b.png)  
위 그림은 여러 초기화 방법들의 성능 비교  
<출처 : 모두를 위한 딥러닝>
