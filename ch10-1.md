# ch10-1. ReLU(Better non-linearity)  

## 1. sigmoid(non-linearity)의 문제점  
### sigmoid -> logistic classification에서 어디에 속하는지 분류하기 위해 사용  
#### 일정 값을 넘어야 True가 될 수 있기 때문에 Activation function이라고도 부름  
  
![9hidden layers](https://user-images.githubusercontent.com/31130917/111309750-96089900-869f-11eb-8cdf-94bfb4db1157.png)  
위 그림은 sigmoid 함수를 이용하여 9개의 hidden layers를 실행  
  
![results](https://user-images.githubusercontent.com/31130917/111309757-9739c600-869f-11eb-9c44-a4ccc5707fdb.png)  
hidden layer를 9개나 두었음에도 불구하고 정확도는 50%  
  
![tensorboard](https://user-images.githubusercontent.com/31130917/111309759-97d25c80-869f-11eb-9427-ce64ad5caaf4.png)  
위 그래프를 보았을 때 cost는 처음에 뚝 떨어진 이후로 변화가 없으나 정확도는 뒤쪽에서 이상하게 변함  
#### => layer가 많은 것은 overfitting 문제의 발생 원인 중 하나
  
![backpropagation](https://user-images.githubusercontent.com/31130917/111310246-38288100-86a0-11eb-8ef6-980d80063ae9.png)  
위 그림처럼 layer가 많을 때는 미분 결과를 처음 layer까지 전달하는 것은 어려움  

    Backpropagation에서 결과를 전달할 때 sigmoid를 사용.  
    그런데 sigmoid는 전달된 값을 0과 1 사이로 심하게 변형함.  
    일정 비율로 줄어들기 때문에 왜곡은 아니지만, 값이 현저하게 작아지는 현상 발생.  
    3개의 layer를 거쳤을때 1/10로 줄어들었다면, 위와 같은 경우 처음 값의 1/1000이 됨.  
    이렇게 작아진 값을 원래대로 그대로 복원한다는 것은 불가능.  
  
![vanishing gradient](https://user-images.githubusercontent.com/31130917/111310250-3959ae00-86a0-11eb-8671-b8468dd1359b.png)  
이와 같은 문제를 Vanishing Gradient라고 부름  
  
## 2. Vanishing Gradient 문제 해결  
### => non-linearity(sigmoid)에 대해 잘못된 방법을 사용(sigmoid 함수가 값을 변형하면서 문제 발생)
  
## 3. ReLU
![relu](https://user-images.githubusercontent.com/31130917/111311411-8db15d80-86a1-11eb-9549-d53f4260f863.png)  
sigmoid 함수 대신 ReLU 함수를 제안  
#### ReLU : 0보다 작을 때는 0을 사용하고, 0보다 큰 값에 대해서는 해당 값을 그대로 사용하는 방법
#### ReLU를 코드로 구현하면 max(0,x)이 됨
  
![relu 식](https://user-images.githubusercontent.com/31130917/111311413-8ee28a80-86a1-11eb-9695-0dace3c99d46.png)  
Tensorflow에서는 sigmoid 함수를 ReLU 함수로 대체  
ReLU 함수로 바꿔서 실행하면 정확도가 훨씬 더 잘 나오는 것을 확인할 수 있음  
  
![activation 함수들](https://user-images.githubusercontent.com/31130917/111312341-850d5700-86a2-11eb-8bcf-155b53b8a011.png)  
위 그림은 다양한 activation 함수들을 보여줌  
#### 가장 좋은 성능을 내는 것은 maxout
  
* #### tanh : sigmoid 함수를 재활용하기 위한 함수, sigmoid 범위를 -1에서 1로 넓힘  
* #### ReLU : max(0,x)처럼 음수에 대해서만 0으로 처리하는 함수  
* #### Leaky ReLU : ReLU 함수의 변형으로 음수에 대해 1/10로 값을 줄여서 사용하는 함수, max(0.1x,x)  
* #### ELU : ReLU를 0이 아닌 다른 값을 기준으로 사용하는 함수  
* #### maxout : 두 개의 W와 b중에서 큰 값이 나온 것을 사용하는 함수  
  
![activation 함수 성능비교](https://user-images.githubusercontent.com/31130917/111312346-863e8400-86a2-11eb-8066-04acc298c365.png)  
위 그림은 Activation 함수의 성능을 비교함  
sigmoid 함수가 n/c라고 나온 것은 결과를 낼 수 없기 때문  
<출처: 모두를 위한 딥러닝>
